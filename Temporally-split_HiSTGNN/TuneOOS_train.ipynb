{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de1292c-ba46-41b3-a279-3bdfec8c3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "from util import *\n",
    "from layer import *\n",
    "from trainer import DoubleTrainer\n",
    "from HierNet import HierarchicalNet\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39848501-dab8-48f0-b631-2b2488b1d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(value):\n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    if value.lower() in {'false', 'f', '0', 'no', 'n'}:\n",
    "        return False\n",
    "    elif value.lower() in {'true', 't', '1', 'yes', 'y'}:\n",
    "        return True\n",
    "    raise ValueError(f'{value} is not a valid boolean value')\n",
    "\n",
    "## --gcn_depth 2 --num_var 4 --var_nodes 4 --stat_nodes 96 --var_node_dim 10 --stat_node_dim 10 --seq_in_len 26 --seq_out_len 1 --batch_size 4 --tanhalpha 2\n",
    "\n",
    "######################################### Illinois settings ###################################\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--device',type=str,default='cpu',help='')\n",
    "parser.add_argument('--dir',type=str,default='',help='data path')\n",
    "parser.add_argument('--data_name',type=str,default='Illinois',help='dataset name')\n",
    "\n",
    "parser.add_argument('--hier_true', type=str_to_bool, default=False, help='whether to use flat graph')\n",
    "parser.add_argument('--DIL_true', type=str_to_bool, default=True, help='whether to use Dynamic Interaction learning')\n",
    "parser.add_argument('--gcn_true', type=str_to_bool, default=True, help='whether to add graph convolution layer')\n",
    "parser.add_argument('--gat_true', type=str_to_bool, default=False, help='whether to add graph attention layer')\n",
    "parser.add_argument('--buildA_true', type=str_to_bool, default=True, help='whether to construct adaptive adjacency matrix')\n",
    "\n",
    "parser.add_argument('--gcn_depth',type=int,default=2,help='graph convolution depth')\n",
    "parser.add_argument('--num_var', type=int, default=4,help='var num during encoding phrase for ablation study') \n",
    "parser.add_argument('--var_nodes',type=int,default=4,help='number of nodes/indices')\n",
    "parser.add_argument('--stat_nodes',type=int,default=96,help='number of nodes/locations')\n",
    "parser.add_argument('--num_heads',type=int,default=4,help='number of multi-head for graph attention') \n",
    "parser.add_argument('--dropout',type=float,default=0.3,help='dropout rate')\n",
    "parser.add_argument('--var_node_dim',type=int,default=20,help='dim of nodes') \n",
    "parser.add_argument('--stat_node_dim',type=int,default=20,help='dim of nodes') \n",
    "parser.add_argument('--dilation_exponential',type=int,default=1,help='dilation exponential')\n",
    "\n",
    "parser.add_argument('--conv_channels',type=int,default=16,help='convolution channels') \n",
    "parser.add_argument('--residual_channels',type=int,default=16,help='residual channels') \n",
    "parser.add_argument('--skip_channels',type=int,default=32,help='skip channels') \n",
    "parser.add_argument('--end_channels',type=int,default=64,help='end channels') \n",
    "\n",
    "parser.add_argument('--in_dim',type=int,default=2,help='inputs dimension')  \n",
    "parser.add_argument('--seq_in_len',type=int,default=26,help='input sequence length')\n",
    "parser.add_argument('--seq_out_len',type=int,default=1,help='output sequence length') ### can be 1 or not\n",
    "\n",
    "parser.add_argument('--layers',type=int,default=3,help='number of layers')\n",
    "parser.add_argument('--batch_size',type=int,default=4,help='batch size')\n",
    "parser.add_argument('--learning_rate',type=float,default=0.001,help='learning rate')\n",
    "parser.add_argument('--propalpha',type=float,default=0.05,help='prop alpha')\n",
    "parser.add_argument('--tanhalpha',type=float,default=1,help='adj alpha')\n",
    "parser.add_argument('--weight_decay',type=float,default=0.0001,help='weight decay rate')\n",
    "parser.add_argument('--clip',type=int,default=5,help='clip')\n",
    "\n",
    "parser.add_argument('--step_size1',type=int,default=1,help='step_size')\n",
    "parser.add_argument('--step_size2',type=int,default=100,help='step_size')\n",
    "\n",
    "parser.add_argument('--epochs',type=int,default=20,help='num of train epoch')\n",
    "parser.add_argument('--print_every',type=int,default=10,help='print information every n iteration')\n",
    "parser.add_argument('--seed',type=int,default=101,help='random seed')\n",
    "#parser.add_argument('--patient',type=int,default=10,help='early stop')\n",
    "\n",
    "parser.add_argument('--expid',type=int,default=1,help='experiment id')\n",
    "parser.add_argument('--runs',type=int,default=1,help='number of runs')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "torch.set_num_threads(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100103c2-9fdb-47b6-bc1a-af7870314f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "illi_data_sepanormal_temporal.dict  is loaded from:  \n",
      "Namespace(device='cpu', dir='', data_name='Illinois', hier_true=False, DIL_true=True, gcn_true=True, gat_true=False, buildA_true=True, gcn_depth=2, num_var=4, var_nodes=4, stat_nodes=96, num_heads=4, dropout=0.3, var_node_dim=20, stat_node_dim=20, dilation_exponential=1, conv_channels=16, residual_channels=16, skip_channels=32, end_channels=64, in_dim=2, seq_in_len=26, seq_out_len=1, layers=3, batch_size=4, learning_rate=0.001, propalpha=0.05, tanhalpha=1, weight_decay=0.0001, clip=5, step_size1=1, step_size2=100, epochs=20, print_every=10, seed=101, expid=1, runs=1)\n",
      "The receptive field size is 16\n",
      "Number of model parameters is 1780870\n",
      "start training...\n"
     ]
    }
   ],
   "source": [
    "runid = 1\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# load data\n",
    "device = torch.device(args.device)\n",
    "dir = args.dir\n",
    "\n",
    "from helper import load_pkl  \n",
    "from util import DataLoaderM \n",
    "\n",
    "def load_Illinois_dataset(dataset_dir, dictname, batch_size, device='cpu'):\n",
    "\n",
    "    ori_data = load_pkl(dataset_dir, dictname)\n",
    "    \n",
    "    data = {}\n",
    "    for category in ['train', 'val', 'test']:\n",
    "        tmpilli = ori_data['x_' + category]\n",
    "        num_year, num_sample, num_variables, num_column = tmpilli.shape  ## num_sample is the number of rows in one year (26*number of counties)\n",
    "\n",
    "        ## reshape x_category\n",
    "        allinlist =[]\n",
    "        num_week = 26\n",
    "        num_county = 96\n",
    "        for eachyear in tmpilli:\n",
    "            for eachweek in eachyear:\n",
    "                for eachindex in eachweek:\n",
    "                    allinlist.append(eachindex[3])       \n",
    "        Yearlist = []\n",
    "        for i in range(1, num_year +1):\n",
    "            x = allinlist[(i-1)*num_sample*num_variables : i*num_sample*num_variables]\n",
    "            Yearlist.append(x)\n",
    "        Countylist = []\n",
    "        for eachyear in Yearlist:    \n",
    "            for i in range(1, num_county +1):\n",
    "                x = eachyear[(i-1)*num_week*num_variables : i*num_week*num_variables]\n",
    "                Countylist.append(x)\n",
    "        Weeklist = []\n",
    "        for ii in range(20, 45+1):\n",
    "            for eachcounty in Countylist:\n",
    "                x = eachcounty[(ii-20)*num_variables : (ii-20+1)*num_variables]\n",
    "                Weeklist.append(x)\n",
    "        Weeksequence = []\n",
    "        for ii in range(1, num_year +1):\n",
    "            for j in range(1, num_week +1):\n",
    "                x = Weeklist[(ii-1)*num_county + (j-1)*num_year*num_county : (ii-1)*num_county + (j-1)*num_year*num_county + num_county]\n",
    "                Weeksequence.append(x)\n",
    "        Yearweeksequence = []\n",
    "        for i in range(1, num_year + 1):\n",
    "            Yearweeksequence.append(Weeksequence[(i-1)*num_week : i*num_week])\n",
    "        x_array = np.array(Yearweeksequence)\n",
    "        tmp = x_array\n",
    "        num_years, num_weeks, num_counties, num_indices = tmp.shape\n",
    "        tmp = np.expand_dims(tmp, axis=-1)\n",
    "        tmp_list = [tmp]\n",
    "        time_ind = [(i % 26) / 26 for i in range(num_weeks)]  # week in year\n",
    "        time_ind = np.array(time_ind) # 26\n",
    "        week_in_year = np.tile(time_ind, [num_years, 1, num_counties, num_indices, 1]).transpose((0, 4, 2, 3, 1))\n",
    "        tmp_list.append(week_in_year)\n",
    "        data['x_' + category] = np.concatenate(tmp_list, axis=-1)\n",
    "\n",
    "        ## reshape y_category\n",
    "        tmp_y = ori_data['y_' + category]\n",
    "        allyieldlist = []\n",
    "        for eachline in tmp_y:    \n",
    "            allyieldlist.append(eachline[2])\n",
    "        Yearyieldlist = []\n",
    "        for ii in range(1, num_years +1):\n",
    "            x = allyieldlist[(ii-1)*num_counties : ii*num_counties]\n",
    "            Yearyieldlist.append(x)\n",
    "        y_array = np.array(Yearyieldlist)\n",
    "        data['y_' + category] = np.expand_dims(y_array, axis=-1) # [year, county, yield]\n",
    "\n",
    "    data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size)\n",
    "    data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], batch_size)\n",
    "    data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], batch_size)\n",
    "\n",
    "    return data\n",
    "    \n",
    "# Illinois\n",
    "dataloader = load_Illinois_dataset(dataset_dir = args.dir, dictname = 'illi_data_sepanormal_temporal.dict', batch_size = args.batch_size, device=args.device)\n",
    "\n",
    "if args.data_name == \"Illinois\":\n",
    "   conv_k_size = (1, 4, 1)\n",
    "\n",
    "\n",
    "model = HierarchicalNet(seq_length=args.seq_in_len, n_var=args.var_nodes, n_stat=args.stat_nodes, var_dim=args.var_node_dim,\n",
    "                        stat_dim=args.stat_node_dim, device=args.device, tanhalpha=args.tanhalpha, conv_channels=args.conv_channels,\n",
    "                        gcn_depth=args.gcn_depth, residual_channels=args.residual_channels, in_dim=args.in_dim,\n",
    "                        dropout=args.dropout, end_channels=args.end_channels, out_dim=args.seq_out_len,\n",
    "                        propalpha=args.propalpha, predefined_A=args.buildA_true, static_feat=None, dilation_exponential=args.dilation_exponential,\n",
    "                        layers=args.layers, layer_norm_affline=True, skip_channels=args.skip_channels,\n",
    "                        gcn_true=args.gcn_true, gat_true=args.gat_true, num_heads = args.num_heads, hier_true=args.hier_true, DIL_true=args.DIL_true, conv_k_size=conv_k_size)\n",
    "\n",
    "print(args)\n",
    "print('The receptive field size is', model.receptive_field)\n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('Number of model parameters is', nParams)\n",
    "\n",
    "engine = DoubleTrainer(model, args.learning_rate, args.weight_decay, args.clip, args.step_size1, args.seq_out_len, device=args.device)\n",
    "\n",
    "print(\"start training...\",flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4053f295-1fec-4dc6-8412-f38ef6a534d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 000, Train Loss: 0.7925, Train MAE: 0.7925, Train MAPE: 1.3832, Train RMSE: 0.8021\n",
      "Epoch: 001, Inference Time: 58.6996 secs\n",
      "Epoch: 001, Train Loss: 0.7925, Train MAPE: 1.3832, Train RMSE: 0.8021, Validation Loss: 0.4946, Validation MAPE: 0.8913, Validation RMSE: 0.5160, Training Time: 29.9671/epoch\n",
      "Iter: 000, Train Loss: 0.6069, Train MAE: 0.6069, Train MAPE: 1.0328, Train RMSE: 0.6184\n",
      "Epoch: 002, Inference Time: 59.4526 secs\n",
      "Epoch: 002, Train Loss: 0.6069, Train MAPE: 1.0328, Train RMSE: 0.6184, Validation Loss: 0.3267, Validation MAPE: 0.5834, Validation RMSE: 0.3587, Training Time: 52.0619/epoch\n",
      "Iter: 000, Train Loss: 0.4308, Train MAE: 0.4308, Train MAPE: 0.7135, Train RMSE: 0.4426\n",
      "Epoch: 003, Inference Time: 56.0386 secs\n",
      "Epoch: 003, Train Loss: 0.4308, Train MAPE: 0.7135, Train RMSE: 0.4426, Validation Loss: 0.2088, Validation MAPE: 0.3745, Validation RMSE: 0.2445, Training Time: 50.2100/epoch\n",
      "Iter: 000, Train Loss: 0.2860, Train MAE: 0.2860, Train MAPE: 0.4744, Train RMSE: 0.3045\n",
      "Epoch: 004, Inference Time: 57.7936 secs\n",
      "Epoch: 004, Train Loss: 0.2860, Train MAPE: 0.4744, Train RMSE: 0.3045, Validation Loss: 0.1905, Validation MAPE: 0.3556, Validation RMSE: 0.2366, Training Time: 41.8778/epoch\n",
      "Iter: 000, Train Loss: 0.2031, Train MAE: 0.2031, Train MAPE: 0.3525, Train RMSE: 0.2240\n",
      "Epoch: 005, Inference Time: 57.8526 secs\n",
      "Epoch: 005, Train Loss: 0.2031, Train MAPE: 0.3525, Train RMSE: 0.2240, Validation Loss: 0.2434, Validation MAPE: 0.4656, Validation RMSE: 0.2941, Training Time: 35.4275/epoch\n",
      "Iter: 000, Train Loss: 0.1938, Train MAE: 0.1938, Train MAPE: 0.3585, Train RMSE: 0.2154\n",
      "Epoch: 006, Inference Time: 60.8968 secs\n",
      "Epoch: 006, Train Loss: 0.1938, Train MAPE: 0.3585, Train RMSE: 0.2154, Validation Loss: 0.2498, Validation MAPE: 0.4803, Validation RMSE: 0.2889, Training Time: 46.0024/epoch\n",
      "Iter: 000, Train Loss: 0.1759, Train MAE: 0.1759, Train MAPE: 0.3663, Train RMSE: 0.2012\n",
      "Epoch: 007, Inference Time: 63.5303 secs\n",
      "Epoch: 007, Train Loss: 0.1759, Train MAPE: 0.3663, Train RMSE: 0.2012, Validation Loss: 0.2025, Validation MAPE: 0.3899, Validation RMSE: 0.2453, Training Time: 49.4184/epoch\n",
      "Iter: 000, Train Loss: 0.1515, Train MAE: 0.1515, Train MAPE: 0.3062, Train RMSE: 0.1734\n",
      "Epoch: 008, Inference Time: 68.4045 secs\n",
      "Epoch: 008, Train Loss: 0.1515, Train MAPE: 0.3062, Train RMSE: 0.1734, Validation Loss: 0.1568, Validation MAPE: 0.2968, Validation RMSE: 0.2022, Training Time: 67.0892/epoch\n",
      "Iter: 000, Train Loss: 0.1407, Train MAE: 0.1407, Train MAPE: 0.2719, Train RMSE: 0.1615\n",
      "Epoch: 009, Inference Time: 61.7154 secs\n",
      "Epoch: 009, Train Loss: 0.1407, Train MAPE: 0.2719, Train RMSE: 0.1615, Validation Loss: 0.1468, Validation MAPE: 0.2716, Validation RMSE: 0.1866, Training Time: 49.4694/epoch\n",
      "Iter: 000, Train Loss: 0.1275, Train MAE: 0.1275, Train MAPE: 0.2302, Train RMSE: 0.1468\n",
      "Epoch: 010, Inference Time: 64.5267 secs\n",
      "Epoch: 010, Train Loss: 0.1275, Train MAPE: 0.2302, Train RMSE: 0.1468, Validation Loss: 0.1452, Validation MAPE: 0.2656, Validation RMSE: 0.1814, Training Time: 38.7966/epoch\n",
      "Iter: 000, Train Loss: 0.1428, Train MAE: 0.1428, Train MAPE: 0.2550, Train RMSE: 0.1650\n",
      "Epoch: 011, Inference Time: 55.9317 secs\n",
      "Epoch: 011, Train Loss: 0.1428, Train MAPE: 0.2550, Train RMSE: 0.1650, Validation Loss: 0.1471, Validation MAPE: 0.2689, Validation RMSE: 0.1798, Training Time: 49.2266/epoch\n",
      "Iter: 000, Train Loss: 0.1195, Train MAE: 0.1195, Train MAPE: 0.2055, Train RMSE: 0.1387\n",
      "Epoch: 012, Inference Time: 54.8103 secs\n",
      "Epoch: 012, Train Loss: 0.1195, Train MAPE: 0.2055, Train RMSE: 0.1387, Validation Loss: 0.1496, Validation MAPE: 0.2751, Validation RMSE: 0.1803, Training Time: 49.4624/epoch\n",
      "Iter: 000, Train Loss: 0.1293, Train MAE: 0.1293, Train MAPE: 0.2424, Train RMSE: 0.1495\n",
      "Epoch: 013, Inference Time: 56.0406 secs\n",
      "Epoch: 013, Train Loss: 0.1293, Train MAPE: 0.2424, Train RMSE: 0.1495, Validation Loss: 0.1552, Validation MAPE: 0.2873, Validation RMSE: 0.1852, Training Time: 52.4537/epoch\n",
      "Iter: 000, Train Loss: 0.1196, Train MAE: 0.1196, Train MAPE: 0.2476, Train RMSE: 0.1380\n",
      "Epoch: 014, Inference Time: 54.8593 secs\n",
      "Epoch: 014, Train Loss: 0.1196, Train MAPE: 0.2476, Train RMSE: 0.1380, Validation Loss: 0.1658, Validation MAPE: 0.3094, Validation RMSE: 0.1958, Training Time: 44.9510/epoch\n",
      "Iter: 000, Train Loss: 0.1150, Train MAE: 0.1150, Train MAPE: 0.2415, Train RMSE: 0.1335\n",
      "Epoch: 015, Inference Time: 55.8137 secs\n",
      "Epoch: 015, Train Loss: 0.1150, Train MAPE: 0.2415, Train RMSE: 0.1335, Validation Loss: 0.1803, Validation MAPE: 0.3379, Validation RMSE: 0.2110, Training Time: 33.9414/epoch\n",
      "Iter: 000, Train Loss: 0.1279, Train MAE: 0.1279, Train MAPE: 0.2366, Train RMSE: 0.1483\n",
      "Epoch: 016, Inference Time: 54.8783 secs\n",
      "Epoch: 016, Train Loss: 0.1279, Train MAPE: 0.2366, Train RMSE: 0.1483, Validation Loss: 0.1827, Validation MAPE: 0.3431, Validation RMSE: 0.2135, Training Time: 42.7753/epoch\n",
      "Iter: 000, Train Loss: 0.1055, Train MAE: 0.1055, Train MAPE: 0.1923, Train RMSE: 0.1223\n",
      "Epoch: 017, Inference Time: 55.0022 secs\n",
      "Epoch: 017, Train Loss: 0.1055, Train MAPE: 0.1923, Train RMSE: 0.1223, Validation Loss: 0.1734, Validation MAPE: 0.3260, Validation RMSE: 0.2035, Training Time: 51.5182/epoch\n",
      "Iter: 000, Train Loss: 0.1057, Train MAE: 0.1057, Train MAPE: 0.2423, Train RMSE: 0.1243\n",
      "Epoch: 018, Inference Time: 55.7128 secs\n",
      "Epoch: 018, Train Loss: 0.1057, Train MAPE: 0.2423, Train RMSE: 0.1243, Validation Loss: 0.1592, Validation MAPE: 0.2992, Validation RMSE: 0.1885, Training Time: 50.2270/epoch\n",
      "Iter: 000, Train Loss: 0.1071, Train MAE: 0.1071, Train MAPE: 0.2247, Train RMSE: 0.1267\n",
      "Epoch: 019, Inference Time: 55.1022 secs\n",
      "Epoch: 019, Train Loss: 0.1071, Train MAPE: 0.2247, Train RMSE: 0.1267, Validation Loss: 0.1434, Validation MAPE: 0.2681, Validation RMSE: 0.1720, Training Time: 41.8838/epoch\n",
      "Iter: 000, Train Loss: 0.0946, Train MAE: 0.0946, Train MAPE: 0.1818, Train RMSE: 0.1102\n",
      "Epoch: 020, Inference Time: 56.0746 secs\n",
      "Epoch: 020, Train Loss: 0.0946, Train MAPE: 0.1818, Train RMSE: 0.1102, Validation Loss: 0.1312, Validation MAPE: 0.2433, Validation RMSE: 0.1599, Training Time: 35.9192/epoch\n"
     ]
    }
   ],
   "source": [
    "## validation loss\n",
    "\n",
    "his_train_loss = []\n",
    "his_valid_loss =[]\n",
    "his_valid_rmse = []\n",
    "his_valid_pred = []\n",
    "train_time = []\n",
    "valid_time = []\n",
    "\n",
    "for i in range(1,args.epochs+1):# epochs   \n",
    "    \n",
    "    train_loss = []\n",
    "    train_mae = []\n",
    "    train_mape = []\n",
    "    train_rmse = []\n",
    "    t1 = time.time()\n",
    "    \n",
    "    dataloader['train_loader'].shuffle() # shuffle train_loader data\n",
    "    \n",
    "    for iter, (x, y) in enumerate(dataloader['train_loader'].get_iterator()):\n",
    "        trainx = torch.Tensor(x).to(device)\n",
    "        trainx = trainx.transpose(1, 4)\n",
    "        trainy = torch.Tensor(y).to(device)\n",
    "        metrics = engine.train(trainx, trainy, iter, data_name=args.data_name)\n",
    "        train_loss.append(metrics[0])\n",
    "        train_mae.append(metrics[1])\n",
    "        train_mape.append(metrics[2])\n",
    "        train_rmse.append(metrics[3])\n",
    "    \n",
    "        if iter % args.print_every == 0 :\n",
    "            log = 'Iter: {:03d}, Train Loss: {:.4f}, Train MAE: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}'\n",
    "            print(log.format(iter, train_loss[-1], train_mae[-1], train_mape[-1], train_rmse[-1]),flush=True)\n",
    "    \n",
    "        t2 = time.time()\n",
    "        train_time.append(t2-t1)\n",
    "    \n",
    "        # validation data\n",
    "        valid_mae = []\n",
    "        valid_mape = []\n",
    "        valid_rmse = []\n",
    "        valid_pred = []\n",
    "    \n",
    "        s1 = time.time()\n",
    "        for iter, (x, y) in enumerate(dataloader['train_loader'].get_iterator()):\n",
    "            validx = torch.Tensor(x).to(device)\n",
    "            validx = validx.transpose(1, 4)\n",
    "            validy = torch.Tensor(y).to(device)\n",
    "\n",
    "            if i !=20 :\n",
    "               metrics = engine.eval(validx, validy, data_name=args.data_name) # first reduce, then expand y:(batch, variables, time)\n",
    "               valid_mae.append(metrics[0])\n",
    "               valid_mape.append(metrics[1])\n",
    "               valid_rmse.append(metrics[2])\n",
    "            else:    \n",
    "               metrics = engine.evalbest(validx, validy, data_name=args.data_name) # first reduce, then expand y:(batch, variables, time)\n",
    "               valid_mae.append(metrics[0])\n",
    "               valid_mape.append(metrics[1])\n",
    "               valid_rmse.append(metrics[2])\n",
    "               valid_pred.append(metrics[3])                    \n",
    "            \n",
    "        s2 = time.time()\n",
    "        log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
    "        print(log.format(i,(s2-s1)))\n",
    "        valid_time.append(s2-s1)\n",
    "    \n",
    "        mtrain_loss = np.mean(train_loss)\n",
    "        mtrain_mape = np.mean(train_mape)\n",
    "        mtrain_rmse = np.mean(train_rmse)\n",
    "        his_train_loss.append(mtrain_loss)\n",
    "    \n",
    "        mvalid_loss = np.mean(valid_mae)\n",
    "        mvalid_mape = np.mean(valid_mape)\n",
    "        mvalid_rmse = np.mean(valid_rmse)\n",
    "        his_valid_loss.append(mvalid_loss)\n",
    "        his_valid_rmse.append(mvalid_rmse)\n",
    "        his_valid_pred.append(valid_pred)\n",
    "    \n",
    "        log = 'Epoch: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, Validation Loss: {:.4f}, ' \\\n",
    "              'Validation MAPE: {:.4f}, Validation RMSE: {:.4f}, Training Time: {:.4f}/epoch'\n",
    "        print(log.format(i, mtrain_loss, mtrain_mape, mtrain_rmse, mvalid_loss, mvalid_mape, mvalid_rmse, (t2 - t1)), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c26948c-23bd-433b-a391-83eb2daae9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "validpredlist = []\n",
    "for eachgroup in his_valid_pred[len(his_valid_pred)-1]:\n",
    "    for eachteam in eachgroup:\n",
    "        for i in range(0,96):\n",
    "            validpredlist.append(eachteam[0][i].item())           \n",
    "validpred = np.array(validpredlist)\n",
    "validpred = validpred[0:3264]\n",
    "\n",
    "## load ytrain\n",
    "yvaliddf = pd.read_excel(\"ytrain.xlsx\")\n",
    "yori_valid = np.array(yvaliddf['Yori'])\n",
    "ynorm_valid = np.array(yvaliddf['Ynorm'])\n",
    "\n",
    "aa_valid = yori_valid\n",
    "bb_valid = (np.max(yori_valid) - np.min(yori_valid))*validpred + np.min(yori_valid)\n",
    "cc_valid = ynorm_valid\n",
    "\n",
    "# train R2\n",
    "expvar1 = np.sum((cc_valid - validpred)**2)\n",
    "expvar2 = np.sum((cc_valid - np.mean(cc_valid))**2)\n",
    "trainR2 = 1 - expvar1/expvar2\n",
    "\n",
    "# train rmse\n",
    "trainrmse = math.sqrt(np.mean(np.square(aa_valid - bb_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd85c99f-b125-411c-8832-4eac3b8b21a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4705217704474519"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc033e1-1455-463b-9f34-d6f62831248b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.75569045839784"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2359c8c-38ea-4e55-8782-4278581db183",
   "metadata": {},
   "outputs": [],
   "source": [
    "storelist = ['Train'] + list(validpred)\n",
    "storelist = np.array(storelist)\n",
    "np.savetxt('Train.txt', storelist, fmt=\"%s\", delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
